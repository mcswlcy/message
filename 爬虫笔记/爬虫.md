# 一、爬虫基础

## 1、安装指令介绍

```python
#安装虚拟环境（window下加-win，苹果系统不需要加）
pip install virtualenvwrapper-win
#常用命令
mkvirtualenv pmz（创建虚拟环境的名字)				 	 #创建虚拟环境成功后会自动切换到环境下
workon envname(envname(切换虚拟环境的名字)) 		  	  #切换到虚拟环境
pip list     										#查看虚拟安装所有的包
deactivate   										#退出虚拟环境
rmvirtualenv pmz(pmz(删除虚拟环境))               	#删除虚拟环境
lsvirtualenv  										#列出所有已创建的虚拟环境
mkvirtualenv--python--C: ..python.exe envname   	#指定Python解释器创建虚拟环境
```



## 2、爬虫概念

```python
爬虫, 又称网页蜘蛛或网络机器人. 
爬虫是 模拟人操作客户端(浏览器, APP) 向服务器发起网络请求 抓取数据的自动化程序或脚本. (*****) 
# 说明: 
	1.模拟: 用爬虫程序伪装出人的行为, 避免被服务识别为爬虫程序 
    2.客户端: 浏览器, APP都可以实现人与服务器之间的交互行为, 应用客户端从服务器获取数据 
    3.自动化: 数据量较小时可以人工获取数据, 但往往在公司中爬取的数据量在百万条, 千万条级别的, 所以要程序		自动化获取数据.
cpython   -   Ipython
#通用爬虫----只是爬虫，为搜索引擎提供检索服务
1.起始url
2.url队列    --------爬取数据


#设计思路
1.确定url，发起请求，获取响应
2.数据解析：re,正则，xpath,Bs4,pyQuery
3.数据持久化：    数据库Mysql   Redisa,MongoDB
```

## 3、爬虫分类

```python
(1): 通用爬虫-----只是爬虫，为搜索引擎提供检索服务：搜索引擎
    # 实例: 百度, 搜狗, Google的搜索引擎 
    # 功能: 访问网页 -> 抓取数据 -> 数据处理 -> 提供检索服务 
    # 工作流:
    	1.给定一个起始URL, 存于爬取队列中 
        2.爬虫程序从队列中取出url, 爬取数据 
        3.解析爬取数据, 获取网页内的所有url, 放入爬取队列
        4.重复第二个步骤
    # 使搜索引擎获取网站链接: 
    	1.主动将url提交给搜索引擎(https://ziyuan.baidu.com/linksubmit/url) 
        2.在其他热门网站设置友情了解 
        3.百度和DNS服务商合作, 收录新网站
    # 网站排名(SEO): 
    	1.根据PageRank值进行排名(流量, 点击率) 
        2.百度竞价排名, 钱多就靠前排 
    # 缺点: 
    	1.抓取的内容多数无用 
        2.无法精确获取数据 
    # 协议: robots协议 --> 约定哪些内容允许哪些爬虫抓取 （约束通用爬虫和聚焦爬虫）
    	1.无需遵守, 该协议适用于通用爬虫, 而我们写的是聚焦爬虫 
        2.查看方法: 网站url/robots.txt, 如https://www.baidu.com/robots.txt
(2): 聚焦爬虫-----聚焦爬虫
    # 概念:
    	聚焦爬虫指针对某一领域根据特定要求实现的爬虫程序, 抓取需要的数据(垂直领域爬取)
    # 设计思路: 
    	(1).确定爬取的url, 模拟浏览器请服务器发送请求  
        (2).获取响应数据并进行数据解析 ---re,正则，[xpath]推荐,Bs4,pyQuery
        (3).将目标数据持久化到本地----- .MySQL，MongoDB，Redis
```

## 4、协议模型

```python
OSI七层模型
7.应用层   6.表示层   5.会话层   4.传输层    3.网络层    2.数据链路层    1.物理层 （从下往上数）
tcp 五层协议
    1.应用层：http/Https/fip/ssh
    2.传输层：tcp/udp
    3.网络层: ip协议
    4.数据链路层：APR协议
    5.物理层：以太网协议
```

##5、HTTP协议与HTTP S协议

```python
# HTTP协议: 明文传输, 端口80 
- Http协议, 全称为Hyper Text Transfer Protocol, 即超文本传输协议. 
- HTTP协议是用于从网络传输超文本数据到本地浏览器的传送协议, 它能保证高效而准确地传送超文本文档. 
- 目前广泛使用的是HTTP 1.1版本     

# HTTPS协议: 加密传输, 端口443 
- HTTPS全称是Hyper Text Transfer Protocol over Secure Socket Layer, 是以安全为目标的 HTTP通道. HTTPS协议实质是HTTP的安全版, 即HTTP下加入SSL安全套接层, 简称HTTPS. 
- HTTPS的安全体现在SSL的加密行为, 即通过HTTPS协议传输的数据都是经过SSL加密的 
- HTTPS的作用: 
    1.建立一个信息安全的通道来保证数据传输的安全 
    2.确认网站的真实性, 凡是使用了HTTPS的网站, 都可以通过点击浏览器地址栏的锁头标志来查看网站 认证之后的真实信息, 也可以通过CA机构颁发的安全签章来查询 
    
# HTTP与HTTPS协议的区别:(背下来) 
	1)、https协议需要到ca申请证书，一般免费证书较少，因而需要一定费用。 
    2)、http是超文本传输协议，信息是明文传输，https则是具有安全性的ssl加密传输协议。 
    3)、http和https使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。 
    4)、http的连接很简单，是无状态的；HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，比http协议安全。
```

## 6、TCP与UDP 协议

 ```python
# TCP与UDP 
1.TCP协议:是一种面向连接的, 可靠的, 基于字节流的传输层通信协议 
	1).有序性: 数据包编号, 判断数据包的正确次序 
	2).正确性: 使用checksum函数检查数据包是否损坏, 发送接收时都会计算校验和 
	3).可靠性: 发送端有超时重发, 并由确认机制识别错误和数据的丢失 
	4).可控性: 滑动窗口协议与拥塞控制算法控制数据包的发送速度 
        
2.UDP协议: 用户数据报协议, 面向无连接的传输层协议, 传输不可靠. 
	1).无连接, 数据可能丢失或损坏 
	2).报文小, 传输速度快 
	3).吞吐量大的网络传输, 可以在一定程度上承受数据丢失

# ARP协议: 通过IP获取目标计算机mac地址的协议（通过IP找mac）
 ```

##7、服务器常见端口

```python
1.ftp: File Transfer Protocol的缩写, 即文件传输协议. 端口:21 
2.ssh: Secure Shell的缩写, 用于远程登录会话. 端口:22 
3.MySQL: 关系型数据库, 端口:3306
4.MongoDB: 非关系型数据库, 端口:27017 
5.Redis: 非关系型数据库, 端口:6379
```

## 8、开发准备

```python
# web端

1. Python3.6 
2. Pycharm 
3. Google Chrome

#抓包工具
fiddler抓包工具

```

# 二、爬虫实践

## 1、第一次爬虫

```python
# b---->代表bytes----->字节类型
# encode()  编码
# decode()  解码   utf-8（国际通用码）   gbk（国标库）    gb2312(国标2312)

'''
#文件使用方式标识
w ---  写，会覆盖所有   r ---读  a --- 写，在末尾追加
wb,rb,ab-------二进制写，读
w+ ---读写，r+读写，区别在于文件不存在，不会创建新文件
'r':默认值，表示从文件读取数据。
'w':表示要向文件写入数据，并截断以前的内容
'a':表示要向文件写入数据，添加到当前内容尾部
'r+':表示对文件进行可读写操作（删除以前的所有数据）
'r+a'：表示对文件可进行读写操作（添加到当前文件尾部）
'b':表示要读写二进制数据
'''
# *args: 动态位置传参
# **kwargs: 动态关键字传参

import requests

'''爬取百度首页'''

# 1.确定url, 向服务器发起请求, 获取响应数据
url = 'https://www.baidu.com/'  
res = requests.get(url=url)
# 2.解析数据(略过)
res.encoding = 'utf-8'
print(res.text)
# 3.持久化到本地: 写文件, MySQL, redis, MongoDB
# f = open('baidu.html', 'w', encoding='utf-8')
# f.write(res.text)
# f.close()
with open('baidu.html', 'w', encoding='utf-8') as f:
    f.write(res.text)
print('执行此行代码前就关闭了文件')
```

## 2、八大请求方式

```python
# 请求: 有客户端向服务器发出的, 可以分为四部分内容: 
	1.请求方法(Request Method), 
    2.请求网址 (Request URL), 
    3.请求头(Request Headers),   #  ******
    4.请求体(Request Body)       #  ******
# 请求方法:常见有8种 (***背会***)
	- GET: 请求页面, 并返回页面内容（**获取**） #重点 
    - POST: 用于提交表单数据或上传文件, 数据包含在请求体中 # 重点 
    - PUT: 从客户端向服务器传送的数据取代指定文档中的内容 
    - DELETE: 请求服务器删除指定的页面 
    - HEAD: 类似于GET请求，只不过返回的响应中没有具体的内容，用于获取报头 
    - CONNECT: 把服务器当作跳板，让服务器代替客户端访问其他网页 
    - OPTIONS: 允许客户端查看服务器的性能 
    - TRACE: 回显服务器收到的请求，主要用于测试或诊断 
************
# 中点掌握GET & POST: GET与POST的区别(重点) --> **(面试出镜率较高)** 
	1.GET请求中的参数包含在URL里面, 数据可以在URL中看到, 而POST请求的URL不会包含这些数据, POST的数据都是通过表单形式传输的, 会包含在请求体中 
    2.GET请求提交的数据最多只有1024字节, 而POST方式没有限制
    
```

## 3、请求头简介

```python
# 请求头: 
请求头，用来说明服务器要使用的附加信息. 重点掌握: ***Accept, Cookie, Referer, User-Agent***       1.Accept：请求报头域，用于指定客户端可接受哪些类型的信息 # 重点(*/*) 
    2.Cookie：也常用复数形式 Cookies，这是网站为了辨别用户进行会话跟踪而存储在用户本地的数据。它 的主要功能是维持当前访问会话。例如，我们输入用户名和密码成功登录某个网站后，服务器会用会话保存登 录状态信息，后面我们每次刷新或请求该站点的其他页面时，会发现都是登录状态，这就是Cookies的功劳。 Cookies里有信息标识了我们所对应的服务器的会话，每次浏览器在请求该站点的页面时，都会在请求头中加 上Cookies并将其发送给服务器，服务器通过Cookies识别出是我们自己，并且查出当前状态是登录状态，所 以返回结果就是登录之后才能看到的网页内容 # 重点 
    3.Referer：此内容用来标识这个请求是从哪个页面发过来的，服务器可以拿到这一信息并做相应的处理，如 作来源统计、防盗链处理等 # 重点 
    ***********************************************
    4.User-Agent：简称UA，它是一个特殊的字符串头，可以使服务器识别客户使用的操作系统及版本、浏览器 及版本等信息。在做爬虫时加上此信息，可以伪装为浏览器；如果不加，很可能会被识别出为爬虫 # 重点 
    ***********************************************
    5.x-requested-with :XMLHttpRequest      #代表ajax请求 
    6.Accept-Language：指定客户端可接受的语言类型 
    7.Accept-Encoding：指定客户端可接受的内容编码 
    8.Content-Type：也叫互联网媒体类型（Internet Media Type）或者MIME类型，在HTTP协议消息头 中，它用来表示具体请求中的媒体类型信息。例如，text/html代表HTML格式，image/gif代表GIF图片， application/json代表JSON类型
```

## 4、反爬与反反爬

```python
1.反爬机制: 
    1).UA检测   2).IP封禁   3).robots协议        4).账号封禁    5).验证码  6).动态数据加载   7).图片懒加载  8).隐藏参数  9).js加密-->js逆向
2.反反爬策略:  
    1).UA伪装   2).IP代理池  3).settings设置     4).cookie池   5).第三发打码平台  6).1.selenium  2.ajax  3.js逆向
```

## 5、常见的状态码

```python
# 响应状态码: 用于判断请求后的相应状态, 如200代表请求成功, 404代表页面页面找不到, 500代表服务 器错误 # 常见的状态码: 
	200系列:
        200 成功 服务器已成功处理了请求 # 重点1 
        
    300系列: 
        301 永久移动 请求的网页已永久移动到新位置，即永久重定向 # 重点 
        302 临时移动 请求的网页暂时跳转到其他页面，即暂时重定向 # 重点 
        
    400系列: 
        400 错误请求 服务器无法解析该请求 # 重点 
        401 未授权 请求没有进行身份验证或验证未通过 
        403 禁止访问 服务器拒绝此请求 # 重点 
        404 未找到 服务器找不到请求的网页 
        
    500系列:
        500 服务器内部错误 服务器遇到错误，无法完成请求 # 重点 
        501 未实现 服务器不具备完成请求的功能 
        502 错误网关 服务器作为网关或代理，从上游服务器收到无效响应 
        504 网关超时 服务器作为网关或代理，但是没有及时从上游服务器收到请求 
        505 HTTP版本不支持 服务器不支持请求中所用的HTTP协议版本
        
**(注意: 状态码不能完全代表响应状态, 部分网站的状态码是自定义的, 一切以响应的数据为准)**
```

##6、响应头

```python
# 响应头: (了解即可)
响应头包含了服务器对请求的应答信息 
	Date：标识响应产生的时间。 
    Content-Encoding：指定响应内容的编码。 
    Server：包含服务器的信息，比如名称、版本号等。 
    Content-Type：文档类型，指定返回的数据类型是什么，如text/html代表返回HTML文档
    application/x-javascript则代表返回JavaScript文件，image/jpeg则代表返回图片。 
    Set-Cookie：设置Cookies。响应头中的Set-Cookie告诉浏览器需要将此内容放在Cookies中，下次请 求携带Cookies请求。 
    Expires：指定响应的过期时间，可以使代理服务器或浏览器将加载的内容更新到缓存中。如果再次访问时， 就可以直接从缓存中加载，降低服务器负载，缩短加载时间。
```

## 7、响应体

```python
# 响应体: --------------重要
最重要的当属响应体的内容了。响应的正文数据都在响应体中，比如请求网页时，它的响应体就是网页的HTML 代码；请求一张图片时，它的响应体就是图片的二进制数据。我们做爬虫请求网页后，要解析的内容就是响应体.
```

## 8、网页基础

```python
# 网页的组成: 
网页可以分为三部分, HTML, CSS, JavaScript 

1.HTML: 其全称叫作Hyper Text Markup Language，即超文本标记语言。 定义了网页的骨架 
2.CSS: 全称叫作Cascading Style Sheets，即层叠样式表。 定义了网页的样式 
3.JavaScript: 简称JS，是一种脚本语言 定义了网页与用户的交互行为, 如下载进度条, 提示框, 轮播图
```

## 9、爬虫工作流

```python
1.确定url, 向服务器发送请求并获得响应: requests, urllib, aiohttp 
2.在响应中提取目标数据, 即数据解析: xpath, bs4, 正则, PyQuery 
3.数据持久化: 文件, 关系型数据库, 非关系型数据库
```

## 10、抓包技能操作认识

```python
***抓包在页面右键检查 或者 F12键
1.箭头---快速定位页面的某个元素（如果在页面定位元素没显示，用快捷键（ctrl+shift+c））
2.小方块---变成手机版的页面
3.Elements---包含页面的所有元素（所有响应数据的集合，也可以快速定位）
4.Network---（重要）网络抓包[Preserve log:保留请求日志，Disable cache:不缓存，缓存提高访问速度]
5.抓包---Headers:包含所有的头部信息，Preview：预览内容，Response:响应内容（搜索：Ctrl+f查询）

```

## 11、requests  模块请求

```python
import requests
#1.确定url，发起请求，获取响应
url = '链接地址'
res = requests.get(url=url)
print(type(res.text))----------输出获取的是一个字符串
with open('zhihu.html','w',encoding='utf-8') as f:
    #res是一个响应对象
    #f.write(字符串)
    f.write(res.text)
    
```

## 12、**requests**   模块基本使用

```python
# requests库的安装 
pip install requests
1.get请求: 
    不携带参数的get请求: 搜狗首页 
    不携带参数的get请求 + headers: 
    爬取知乎的发现页 携带参数的get请求 + headers: 知乎的发现栏中搜索Python 
    # parameters 的缩写 params--{}字典的形式--拼接再url?后面(也可以封装函数，传参)
    res = requests.get(url=url, headers=headers, params=params，proxies=proxies) 
    
2.post请求: 构建参数的post请求 
    # data 数据(在网页Form Data里)响应数据是json数据类型--字典{}
    res = requests.post(url=url, headers=headers, data=data,proxies=proxies)
    
3. json形式与流形式的响应数据示例

	json.dumps	将 Python 对象编码成 JSON 字符串
	json.loads	将已编码的 JSON 字符串解码为 Python 对象
    
	# json形式响应数据示例: bilibili的Python视频教程, 目录列表 
    import requests 
    headers = { 'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36' }
    url = 'https://api.bilibili.com/x/web-interface/view?aid=14184325&cid=23153678' 	res = requests.get(url=url, headers=headers) 
    print(res) 
    print(res.status_code) 
    print(res.headers) 
    print(res.json())
4.响应数据的获取与属性 
	(1).响应数据的获取: 
        res.text: 文本数据 
        res.json(): json数据 ------->dict
        res.content: 流, 图片, 视频, 压缩包, 软件包 
    (2).响应的其他属性:
        res.status_code: 获取响应状态码 
        res.headers: 响应头
        res.cookie: cookie信息
        res.history: 历史
```

## 13、代理 IP

```python
#goubanjia.com   IP代理商
#proxy   变成了  proxies的参数{}
正向代理：对于浏览器知道服务器的真实地址，例如VPN
反向代理：浏览器不知道服务器的真实地址，例如nginx

代理IP的分类
根据代理ip的匿名程度，代理IP可以分为下面四类：

透明代理(Transparent Proxy)：透明代理虽然可以直接“隐藏”你的IP地址，但是还是可以查到你是谁。
匿名代理(Anonymous Proxy)：使用匿名代理，别人只能知道你用了代理，无法知道你是谁。
高匿代理(Elite proxy或High Anonymity Proxy)：高匿代理让别人根本无法发现你是在用代理，所以是最好的选择。

从请求使用的协议可以分为：

http代理
https代理
socket代理等
不同分类的代理，在使用的时候需要根据抓取网站的协议来选择
import random 
ip-list = ['']
proxies = {
    'https':'http:// %s' % random.choice(ip-list)
}
```

# 三、requests 高级

## 1、文件上传功能

```python
#用的少---应用场景（可以当时上传文件）
import requests 
# 定义上传文件数据, 键为file, 值为文件句柄
files = {
'file': open('favicon.ico', 'rb')
}
#指定网址，指定字段
r = requests.post('http://httpbin.org/post', files=files)
print(res.text)
会话***代理设置
```

## 2、cookie处理*

```python
# cookie处理方式
	1.headers添加cookie键值对 ---->Session
    2.RequestsCookieJar对象
```

```python
#2. RequestsCookieJar对象处理cookie: 用cookie维持百度登陆
#爬知乎
import requests
from requests.cookies import RequestsCookieJar
cookies = 'BAIDUID=79A570F8D90B2C45E42D40A3666ADC46:FG=1; BIDUPSID=79A570F8D90B2C45E42D40A3666ADC46; PSTM=1551074009; BD_UPN=12314753; sugstore=0; BDORZ=FFFB88E999055A3F8A630C64834BD6D0; yjs_js_security_passport=10c9ca61409abe70ac5c03db796f78648e697d8f_1563711806_js; COOKIE_SESSION=2860_2_2_7_3_5_0_0_2_4_106_0_3778_177561_116_109_1563714759_1563714752_1563714643%7C9%23177557_14_1563714643%7C7; delPer=0; BD_HOME=0; H_PS_PSSID=1452_21117_29522_29521_28519_29099_28831_29221; BDUSS=lSVnBVVkRVNFpNZ2ZJZ2ZpNFpjblFFSX5EaW9DNzBpcnNkaDZIQVdRd2Z1bHhkRVFBQUFBJCQAAAAAAAAAAAEAAABwfMtW09rQodPjMDgyMGZyZWUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAB8tNV0fLTVdYX'

headers = {
    'User-Agetn': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36'
}
#实例化对象
jar = RequestsCookieJar()
#分割
cookie_list = cookies.split(';')
for cookie in cookie_list:
    #再以等号分割
    key, value = cookie.split('=', 1)
    
    jar.set(key, value)
    
res = requests.get('http://www.baidu.com', cookies = jar, headers=headers)
print(res.text)  # 响应数据中包含用户名信息, 说明cookie生效
```

## 3、会话维持与模拟登陆

```python
# HTTP无状态:
	使用requests模块中的get()和post()方法请求网页时, 每一次请求都是独立的, 没有连续请求之间的状态保持. 假象, 如果你登陆了淘宝后向查看订单, 那么如果没有状态的维持就无法实现.
    
# 会话的维持: Session对象
from requests import Session
s = Session()
res = s.get('https://www.baidu.com')
	#PyExecJS
    #js2py    加密
# 人人网登陆案例:
from requests import Session 
session = Session() 
url = 'http://www.renren.com/ajaxLogin/login?1=1&uniqueTimestamp=2019761744568' headers = { "USer-Agent":'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36' }data = { 'email': '17679962330', 'autoLogin': 'true', 'icode': '', 'origURL': 'http://www.renren.com/home', 'domain': 'renren.com', 'key_id': '1', 'captcha_type': 'web_login', 'password': '6ea935849c9dbfc4ac484718ac8652a14f4b2f60036de7a279e84be08bc54136', 'rkey': '1c7df63368df7ce73c234de26178ec11', 'f': 'http%3A%2F%2Fwww.renren.com%2F972036549%2Fnewsfeed%2Fphoto', }
res = session.post(url=url, data=data, headers=headers) 
ret = session.get(url='http://www.renren.com/972036549/profile', headers=headers) ret.encoding = 'utf-8' 
with open('renren.html', 'w', encoding='utf-8') as f: 
    f.write(ret.text)
```

## 4、SSL证书验证

```python
# 1.SSL证书验证
requests提供了证书验证的功能. 当发起HTTP请求时, 模块会检查SSL证书. 但检查的行为可以用verify参数来控制.
	verify = False  # 不检查SSL证书
	verify = True  # 检查SSL证书
    
# 2.异常
如果使用requests模块的SSL验证, 验证不通过会抛出异常, 此时可以将verify参数设置为False

# 3.www.12306.cn的证书验证
# 会抛出异常
import requests
response = requests.get('https://www.12306.cn')
print(response.status_code)

# 不抛异常, 但会出现警告
import requests
response = requests.get('https://www.12306.cn', verify=False)
print(response.status_code)

# 异常: SSLError 
requests.exceptions.SSLError        #证书错误

# 禁止警告
import requests
from requests.packages import urllib3
urllib3.disable_warnings()
response = requests.get(url='https://www.12306.cn', verify=False)
print(response.status_code)
```

## 5、代理设置

```python
# 代理: 代理即代理ip 
代理ip是指在请求的过程中使用非本机ip进行请求, 避免大数据量频繁请求的过程中出现ip封禁, 限制数据 的爬取. 

# 代理ip分类: 
	1.透明代理ip: 请求时, 服务器知道请求的真实ip, 知道请求使用了代理 
    2.匿名代理ip: 请求时, 服务器知道请求使用了代理, 但不知道请求的真实ip 
    3.高匿代理ip: 请求时, 服务器不知道请求使用了代理, 也不知道请求的真实ip 
    #基于隧道：云端维护了一个庞大的IP代理池，每次请求换一个IP
    #提供接口：返回一部分数量的IP，配合IP代理池使用
        
# requests模块使用代理ip 
import requests 
url = 'http://www.httpbin.org'
proxies = { 'http': 'http://61.183.176.122:57210' }
res = requests.get(url=url, proxies=proxies) 
print(res.text)
```

## 6、超时设置

```python
# 超时设置: 
	由于网络状况的不同, 服务器配置差异以及服务器处理并发的能力不同, 有时会出现服务器的响应时间 过长, 甚至无法获取响应而抛出异常. requests模块发送请求可以设置超时时间, 在超时时间内未得到响 应, 便会抛出异常. 
	一方面, 减少了请求的阻塞时间, 一方面, 可以进行异常处理, 执行相应的操作. 
import requests 
url = 'https://www.baidu.com' 
headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36' }
res = requests.get(url=url, headers=headers, timeout=0.001) 
# 在0.001秒为得到响 应, 抛出requests.exceptions.ConnectTimeout异常 
print(res.text)
```

## 7、构建Request对象

```python
# 1.Prepared Request 
利用 Prepared Request 数据结构构建Request对象. 其构建及使用步骤如下: 
from requests import Request, Session 
# 构建Request对象 
url = '...' 
data = {... }
params = {... }
headers = {... }
session = Session() 
# 构建post请求: 
req_post = Request(method='POST', url=url, headers=headers, data=data) 
req_obj_post = session.prepare_request(req_post)

# 构建get请求: 
req_get = Request(method='GET', url=url, headers=headers, params=params) 
req_obj_get = session.prepare_request(req_get) 
# 利用构建的请求对象, 向服务器发送请求 
res = session.send(req_obj_post) 
res = session.send(req_obj_get) 
# 应用: 通过此方法, 我们可以构建一个独立的request对象, 当需要请求的url很多时, 我们可以为每一个url构建 一个request对象, 将所有request对象置于队列中, 便于调度.

# 构建request对象, 请求糗事百科获取页面 
from requests import Request, Session 
url = 'https://www.qiushibaike.com/' 
headers = { "User-Agent":'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36' }
session = Session() 
req_get = Request(url=url, headers=headers, method='GET') 
req_get_obj = session.prepare_request(req_get) 
res = session.send(req_get_obj) 
res.encoding = 'utf-8' 
with open('qb_reqobj.html', 'w', encoding='utf-8') as f: 
    f.write(res.text)
```

## 8、urllib简单介绍

```python
# urllib简介: 
1.urllib模块是Python的一个请求模块 
2.Python2中是urllib和urllib2相结合实现请求的发送. Python3中统一为urllib库 
3.urllib是Python内置的请求库, 其包含4个模块: 
	(1).request模块: 模拟发送请求 
	(2).error模块: 异常处理模块 
	(3).parse模块: 工具模块, 提供关于URL的处理方法, 如拆分, 解析, 合并等 
	(4).robotparser模块: 识别robots协议
```

```python
# 部分方法使用介绍:
# urlopen方法实现get请求: 
from urllib import request 
url = 'https://www.python.org' 
res = request.urlopen(url) 
print(res.read()) 
with open('python.html', 'w') as f: 
    f.write(res.read().decode('utf-8'))
    
# post请求: 
import urllib.request 
import urllib.
parse url='https://fanyi.baidu.com/sug' postdata=urllib.parse.urlencode({'kw':'boy'}).encode('utf-8') 
res = urllib.request.urlopen(url, data=postdata) 
print(res.read())

# urlretrive实现图片下载: 
from urllib.request import urlretrieve urlretrieve('https://www.dxsabc.com/api/xiaohua/upload/min_img/20190213/20190213 XTUcIZ99B9.jpg', 'bing.jpg')
```

## 9、正则介绍

```python
# 1.元字符匹配 
. 匹配任意字符，除了换行符(重要) 
[] 用来表示一组字符,单独列出：[abc] 匹配 'a'，'b'或'c' 
[^...] 匹配除了字符组中字符的所有字符 
\d 匹配任意数字，等价于 [0-9]. 
\D 匹配任意非数字 
\w 匹配字母数字及下划线 
\W 匹配非字母数字及下划线

\s 匹配任意空白字符，等价于 
[\t\n\r\f]. \S 匹配任意非空字符

# 2.字符组: 要求在一个位置匹配的字符可能出现很多种情况, 各种情况组成一个组 
[0123456789]: 匹配0到9任意字符 
[0-9]: 同上 [a-z]: 匹配a到z的任意小写字母 
[A-Z]: 匹配A到Z的任意大写字母 
[0-9a-fA-F]: 以上三种的组合, 匹配0-9任意数组或a到f之间任意字母, 不区分大小写 
自定义字符组:[a3h5] ---> 代表匹配a, 3, h, 5等字符
    
# 3.量词: 
* 重复零次或更多次 
+ 重复一次或更多次 
? 非贪婪匹配 
{n} 重复n次 
{n,} 重复n次或更多次 
{n,m} 重复n到m次 
{,m} 重至多m次

# 4.边界修饰符 
^ 匹配开始 
$ 匹配结尾

# 5.分组(重点, 重点, 重点) 
在正则表达式中添加(), 就形成了一个分组, 在re模块中优先匹配显示分组内容 
import re 
s = "<a href='www.baidu.com'>正则匹配实验</a>" 
res = re.findall("href='(.*)'>", s) 
print(res)

# 6.贪婪匹配与非贪婪匹配 
贪婪匹配是指: 在使用量词: * , + 等时, 尽可能多的匹配内容 
非贪婪匹配是指: 使用?对正则表达式进行修饰, 使量词的匹配尽可能少, 如+代表匹配1次或多次, 在?的修 饰下, 只匹配1次.
    
# 7.匹配模式 
re.S 单行模式(重点) 
re.M 多行模式 
re.I 忽略大小写 
# 示例: 
import re 
s = 'hello2world\nhello3world\nhello4world' 
#re.M 多行模式 
result0 = re.findall(r'\d.*d', s) 
print(result0) 
result1 = re.findall(r'\d.*d', s, re.M) 
print(result1) 
#re.S 单行模式(可以看成将所有的字符串放在一行内匹配包括换行符\n) 
result2 = re.findall(r'\d.*d', s, re.S)
print(result2) 
result3 = re.findall(r'\d.*?d', s, re.S) 
print(result3)

# 8.re模块 
1.re.findall('正则表达式', '待匹配字符串'): 返回所有满足匹配条件的结果, 以列表形式返回 2.re.search('正则表达式', '带匹配字符串'): 匹配到第一个就返回一个对象, 该对象使用group()进 行取值, 如果未匹配到则返回None 
3.re.match('正则表达式', '待匹配字符串'): 只从字符串开始进行匹配, 如果匹配成功返回一个对象, 同样使用group()进行取值, 匹配不成功返回None 
4.re.compile('正则表达式'): 将正则表达式编译为对象, 在需要按该正则表达式匹配是可以在直接使用 该对象调用以上方法即可. 
    
Python语言: 解释型语言 
    先解释在执行: 源代码 --> 简单的翻译 --> 字节码 --> 二进制语言 --> 识别的语言 
    .pyc文件: 执行过的文件, 生成一个.pyc文件, 再执行时对比. 
C: 编译型语言 源代码 ---> 编译 ---> 二进制文件 --> 识别的语言 
    
# 示例: 
import re 
s = "pythonpython你好吊" 
# findall方法演示 
res_findall = re.findall(r'p', s) 
print('findall匹配结果:', res_findall)

# search方法演示, 不确定是否能匹配出结果, 不可直接使用group进行取值, 需要判断或进行异常处理 res_search = re.search(r"你", s) 
if res_search: 
    print('search匹配结果', res_search.group()) 
else:
    print('None') 
    
# match方法演示: 
res_match_1 = re.match(r'py', s) 
res_match_2 = re.match(r'thon', s) 
print('res_match_1结果:', res_match_1) 
print('res_match_2结果:', res_match_2) 
# compile方法演示: 
re_obj = re.compile(r'python') 
res = re.findall(re_obj,s) print(res)
```

## 10、校花网图片爬取与多页爬取

```python
import re
import requests
for j in range(2):
    # url = 'http://www.xiaohuar.com/list-1-'+str(j)+'.html'
    #获取当前网页地址
    url = 'http://www.xiaohuar.com/list-1-%s.html'% j
    # 头部伪装
    headers = {
        'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.132 Safari/537.36'
    }
    #获取响应，以文本文档的形式输出
    res = requests.get(url=url)
    # print(res.text)
    #用findall的正则匹配（src="(\d.*?\.jpg)/></a>"）,符合条件的都存入列表
    ret = re.findall(r'src="(\d.*?\.jpg)" /></a>',res.text)
    # print(ret)
    # 循环遍历
    for i in ret:
        #每张图片都配置个地址
        url = 'http://www.xiaohuar.com'+i
        #获取响应以二进制流写入
        ret1 = requests.get(url=url,headers=headers).content
        name = i.split('/')[-1]
        #以文件打开方式存入img文件夹中
        with open('../img/'+name,'wb') as f:
            f.write(ret1)

```

# 四、xpath的解析，应用

##1、Xpath解析库介绍: 

```python
# Xpath解析库介绍: 
	数据解析的过程中使用过正则表达式, 但正则表达式想要进准匹配难度较高, 一旦正则表达式书写错误, 匹配的数据也会出错. 
	网页由三部分组成: HTML, Css, JavaScript, HTML页面标签存在层级关系, 即DOM树, 在获取目 标数据时可以根据网页层次关系定位标签, 再获取标签的文本或属性.
        
# xpath解析库解析数据原理: 
1. 根据网页DOM树定位节点标签 
2. 获取节点标签的正文文本或属性值

# xpath安装, 初体验 --> 使用步骤: 
1.xpath安装: pip install lxml 
2.requests模块爬取糗事百科热门的标题: 
    
import requests 
from lxml import etree 

url = 'https://www.qiushibaike.com/' 
headers = { "User-Agent":'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36' }
res = requests.get(url=url, headers=headers) 
#实例化对象
tree = etree.HTML(res.text) 
#解析数据
title_lst = tree.xpath('//ul/li/div/a/text()') 
for item in title_lst: 
    print(item) 
    
3.xpath使用步骤: 
from lxml import etree
tree = etree.HTML(res.text) 
tree = etree.parse(res.html, etree.HTMLParse()) # 示例如下, 了解内容 tag_or_attr = tree.xpath('xpath表达式')
```

## 2、xpath语法

```python
# xpath语法: 
1.常用规则: 
	1. nodename: 节点名定位 
	2. //: 从当前节点选取子孙节点 
	3. /: 从当前节点选取直接子节点 
	4. nodename[@attribute="..."] 根据属性定位标签 '//div[@class="ui-main"]' 
	5. @attributename: 获取属性 
	6. text(): 获取文本 
2.属性匹配两种情况: 多属性匹配 & 单属性多值匹配 
	2.1 多属性匹配 示例: tree.xpath('//div[@class="item" and @name="test"]/text()') 
	2.2 单属性多值匹配 示例: tree.xpath('//div[contains(@class, "dc")]/text()') 
3.按序选择: 
	3.1 索引定位: 从1开始(牢记， 牢记， 牢记) 
	3.2 last()函数 
	3.3 position()函数 ---代表几个以内>,<  位置
```

## 3、xpath代码演示

```python
from lxml import etree

# 1.实例化一个etree对象
# tree = etree.HTML('文本数据')    # 解析直接从网络上爬取内容
# reel = etree.parse('文本数据',etree.HTMLParser())   # 解析本地的HTML文本
reel = etree.parse('./test.html',etree.HTMLParser())   # 解析本地的HTML文本

#2.调用 xpath 表达式定位标签及获取其属性与文本
#2.1根据节点定位

title = reel.xpath('//title/text()')   #xpath匹配出来是一个列表
# print(title)

# 3. 定位id为007的标签，去直接文本
div_oo7 = reel.xpath('//div[@id="007"]/text()')
# print(div_oo7)

div_008 = reel.xpath('//div[@id=007]//text()')
# print(div_008)

# 4.获取节点的属性值
a_tag = reel.xpath('//a/@href')
# print(a_tag)

# 5.多属性匹配和单属性多值匹配
# 多属性匹配
div_009 = reel.xpath('//div[@class="c1" and @name="laoda"]/text()')
# print(div_009)

# 单属性多值匹配
div_010 = reel.xpath('//div[contains(@class,"c3")]/text()')
# print(div_010)

#6、按序匹配
div_011 = reel.xpath('//div[@class="divtag"]/ul/li/text()')
# print(div_011)

div_012 = reel.xpath('//div[@class="divtag"]/ul/li[4]/text()')
# print(div_012)

# div_013 = reel.xpath('//div[@class="divtag"]/ul/li[last()-1]/text()')
# print(div_013)

div_014 = reel.xpath('//div[@class="divtag"]/ul/li[position()<4]/text()')
print(div_014)

```

## 4、豆瓣案例

```python
import requests
from lxml import etree
url = 'https://movie.douban.com/chart'
headers = {
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.132 Safari/537.36'
}
res = requests.get(url=url,headers=headers)
tree = etree.HTML(res.text)
ret = tree.xpath('//div[@class="pl2"]')
for i in ret:
    title = i.xpath('./a//text()')
    title_full = ''
    for j in title:
        c = j.replace('\n','').replace(' ','')
        title_full += c
    author = i.xpath('./p//text()')
    pj = i.xpath('./div/span[2]/text()')
    pf = i.xpath('./div/span[3]/text()')
    print(title_full)
    print(author[0])
    print(pj[0])
    print(pf[0])

```

# 五、存入三大文件

```python
import requests
import json,csv
from lxml import etree
for i in range(1,10):
    if i == 1:
        url = 'http://www.lnzxzb.cn/gcjyxx/004001/subpage.html'
    else:
        # url = 'http://www.lnzxzb.cn/gcjyxx/004001/%s.html' % i
        url = 'http://www.lnzxzb.cn/gcjyxx/004001/'+str(i)+'.html'
    headers = {
        'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.132 Safari/537.36'
    }
    res = requests.get(url=url,headers=headers)
    tree = etree.HTML(res.text)

    #存 txt  文件       ***********************************
    # with open('ztb.txt', 'a', encoding='utf-8') as f:
    #     for i in range(1,16):
    #         ret = tree.xpath('//ul[@id="showList"]/li['+str(i)+']/p/a/@href')[0]
    #         ret1 = tree.xpath('//ul[@id="showList"]/li['+str(i)+']/p/a/@title')[0]
    #         ret2 = tree.xpath('//ul[@id="showList"]/li['+str(i)+']/span[1]/text()')[0]
    #         # print(ret+ret1+ret2)
    #         f.write(''.join([ret,ret1,ret2,'\n']))

    
    #	存 json 文件 **************************
    # with open('ztb.json', 'a', encoding='utf-8') as f:
    #     for i in range(1,16):
    #         ret = tree.xpath('//ul[@id="showList"]/li['+str(i)+']/p/a/@href')[0]
    #         ret1 = tree.xpath('//ul[@id="showList"]/li['+str(i)+']/p/a/@title')[0]
    #         ret2 = tree.xpath('//ul[@id="showList"]/li['+str(i)+']/span[1]/text()')[0]
    #         # print(ret+ret1+ret2)
    #         dic = {'ret':ret,'ret1':ret1,'ret2':ret2}
    #         f.write(json.dumps(dic,indent=4,ensure_ascii=False)+',')

    #存 CSV 文件---导包 import csv  ***************************
    with open('ztb.csv', 'a', encoding='utf-8') as f:
        # delimiter=' '  必须是一个字符，一个空格,或者逗号
        # writer  俩个参数
        wr = csv.writer(f,delimiter=',')
        # writerow---先写入CSV文件，定义格式
        wr.writerow(['link','title','times'])
        for i in range(1,16):
            ret = tree.xpath('//ul[@id="showList"]/li['+str(i)+']/p/a/@href')[0]
            ret1 = tree.xpath('//ul[@id="showList"]/li['+str(i)+']/p/a/@title')[0]
            ret2 = tree.xpath('//ul[@id="showList"]/li['+str(i)+']/span[1]/text()')[0]
            # print(ret+ret1+ret2)
            
            wr.writerow([ret,ret1,ret2])
```

# 六、BeautifulSoup库使用

##1、BeautifulSoup库介绍: 

```python
# BeautifulSoup库介绍: 
BeautifulSoup也是一个解析库 
BS解析数据是依赖解析器的, BS支持的解析器有html.parser, lxml, xml, html5lib等, 其中lxml 解析器解析速度快, 容错能力强. 
BS现阶段应用的解析器多数是lxml
```

## 2、BeautifulSoup 使用步骤:

```python
# BeautifulSoup 使用步骤: 
from bs4 import BeautifulSoup 
soup = BeautifulSoup(res.text, 'lxml')   #实例化BeautifulSoup对象
tag = soup.select("CSS选择器表达式") # 返回一个列表
```

## 3、选择器分类: 

```python
1).节点选择器 2).方法选择器 3).CSS选择器
```

## 4、 CSS选择器: 

```python
# CSS选择器:
1.根据节点名及节点层次关系定位标签: 标签选择器  &  层级选择器
soup.select('title')
soup.select('div > ul > li')   # 单层级选择器
soup.select('div li')  # 多层级选择器


2.根据节点的class属性定位标签: class选择器(classical:经典)
soup.select('.panel')


3.根据id属性定位标签: id选择器
soup.select('#item')


4.嵌套选择:
ul_list = soup.select('ul')   #得到的依然是一个数据列表
for ul in ul_list:
  print(ul.select('li'))

# 获取节点的文本或属性:
# 如果标签下除了直接子文本外还有其他标签，string将无法获取直接子文本
tag_obj.string:         #获取直接子文本-->如果节点内有与直系文本平行的节点, 该方法拿到的是None
tag_obj.get_text(): 	#获取子孙节点的所有文本
tag_obj['attribute']:   #获取节点属性
```

```python
# 练习示例:
from bs4 import BeautifulSoup
html = '''
    <div class="panel">
        <div class="panel-heading">
            <h4>BeautifulSoup练习</h4>
        </div>
        <div>
        	这是一个div的直接子文本
        	<p>这是一个段落</p>
        </div>
        <a href="https://www.baidu.com">这是百度的跳转连接</a>
        <div class="panel-body">
            <ul class="list" id="list-1">
                <li class="element">第一个li标签</li>
                <li class="element">第二个li标签</li>
                <li class="element">第三个li标签</li>
            </ul>
            <ul class="list list-small">
                <li class="element">one</li>
                <li class="element">two</li>
            </ul>
            <li class="element">测试多层级选择器</li>
        </div>
    </div>
'''
# 1.实例化BeautifulSoup对象
soup = BeautifulSoup(html,'lxml')
#2.条用css选择器定位标签，获取标签的文本或属性

### 2.1 根据节点名定位
# r1 = soup.select('h4')
# print(type(r1))
# #列表用下标0取出第一个元素
# print(r1[0].string)         #直接获取子文本
# print(r1[0].get_text())     #直接获取所有的子孙文本

### 2.2 根据节点的class定位
# k1 = soup.select('.panel-heading')
# print(k1[0])

### 2.3根据id选择标签定位
# k2 = soup.select('#list-1')
# print(k2[0].get_text())

### 2.4单层级选择器
# cc = soup.select('.panel-body > ul >li')
# print(cc)

### 2.5多层级选择器
# cc1 = soup.select('.panel-body li')
# print(cc1)
```

```python
'''
利用bs4语法，爬取三国演义文字
'''''
# import requests
# from bs4 import BeautifulSoup
#
# url = 'http://www.shicimingju.com/book/sanguoyanyi.html'
# headers = {
#     'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.132 Safari/537.36'
# }
# res = requests.get(url=url,headers=headers)
# soup = BeautifulSoup(res.text,'lxml')
# ret = soup.select('.book-mulu ul li')
# for i in ret:
#     title = i.select('a')[0].string
#     comment = 'http://www.shicimingju.com'+i.select('a')[0]['href']
#     ret1 = requests.get(url=comment,headers=headers)
#     res1 = BeautifulSoup(ret1.text,'lxml')
#     cc = res1.select('.chapter_content ')[0].get_text()
#     with open('threecountry.txt','a',encoding='utf-8') as f:
#         f.write(cc+'\n')

'''
利用xpath语法，爬取三国演义文字
'''''
import requests
from lxml import etree
url = 'http://www.shicimingju.com/book/sanguoyanyi.html'
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.132 Safari/537.36'
}
res = requests.get(url=url,headers=headers)
tree = etree.HTML(res.text)
ret = tree.xpath('//div[@class="book-mulu"]/ul/li')
for i in ret:
    rec = i.xpath('./a/@href')[0]
    name = i.xpath('./a/text()')[0]
    url = 'http://www.shicimingju.com'+rec
    res1 = requests.get(url= url,headers=headers)
    tree1 = etree.HTML(res1.text)
    cope = tree1.xpath('//div[@class="chapter_content"]/p/text()')[0]+'\n'
    with open(name+'.txt','a',encoding='utf-8') as f:
        f.write(cope)
```

# 七、selenium框架

## 1、selenium介绍

```python
# 介绍: 
1.selenium是一个web自动化测试用的框架. 程序员可以通过代码实现对浏览器的控制, 比如打开网页, 点 击网页中的元素, 实现鼠标滚动等操作. 
2.它支持多款浏览器, 如谷歌浏览器, 火狐浏览器等等, 当然也支持无头浏览器. 
# 目的: 
在爬取数据的过程中, 经常遇到动态数据加载, 一般动态数据加载有两种, 一种通过ajax请求加载数据, 另 一种通过js代码加载动态数据. selenium可以模拟人操作真实浏览器, 获取加载完成的页面数据

ajax:
    url有规律且未加密, 直接构建url连接请求 
    url加密过无法破解规律 --> selenium 
js动态数据加载 --> selenium
```

## 2、selenium安装

```python
三要素: 浏览器, 驱动程序, selenium框架 
    浏览器: 推荐谷歌浏览器, 标准稳定版本 驱动程序:http://chromedriver.storage.googleapis.com/index.html pip install selenium 
                
# 测试: 
from selenium import webdriver 
browser = webdriver.Chrome('./chromedriver.exe') # 将驱动放在脚本所在的文件夹 
browser.get('https://www.baidu.com')
```

## 3、selenium常用操作

```python
# 实例化浏览器对象: 
from selenium import webdriver 
browser = webdriver.Chrome('driverpath') 

# 发送get请求: 
browser.get('https://www.baidu.com')
browser.get('https://image.baidu.com')
# 获取网页的数据: browser.page_source ---> str类型 
# 获取页面元素: 
find_element_by_id:根据元素的id 
find_element_by_name:根据元素的name属性 find_element_by_xpath:根据xpath表达式 find_element_by_class_name:根据class的值 find_element_by_css_selector:根据css选择器
                
# 节点交互操作: 
click(): 点击 
send_keys(): 输入内容 
clear(): 清空操作 
execute_script(js): 执行指定的js代码 
# JS代码: window.scrollTo(0, document.body.scrollHeight)可以模拟鼠标滚动一屏高度 
quit(): 退出浏览器 


# frame 
# 若爬取一个页面，需观察如果有两个HTML（一个父HTML，一个子HTML），所需内容在子HTML中，则需要switch_to.frame('frameid') 转至子页面继续操作

switch_to.frame('frameid')                
```

## 4、QQ空间模拟登陆

```python
from selenium import webdriver
import time
# 实例化浏览器对象
browser = webdriver.Chrome('./chromedriver.exe')
# 打开qq空间登陆页面
browser.get('https://qzone.qq.com/')
time.sleep(1)
# 转至frame子页面
browser.switch_to.frame('login_frame')
# 获取密码登陆选项并点击
a_tag = browser.find_element_by_id('switcher_plogin')
a_tag.click()
time.sleep(1)
# 获取账号输入框并输入账号
browser.find_element_by_id('u').clear()
user = browser.find_element_by_id('u')
user.send_keys('1816668038')
time.sleep(1)
# 获取密码输入框并输入密码
browser.find_element_by_id('p').clear()
pwd = browser.find_element_by_id('p')
pwd.send_keys('1971628197192liu')
time.sleep(1)
# 获取登陆按钮并单击
button = browser.find_element_by_id('login_button')
button.click()




'''
微博模拟登陆
'''''
# import requests,time
# from selenium import webdriver
# broeser = webdriver.Chrome('./chromedriver.exe')
# broeser.get('https://weibo.com/login.php')
#
# input_tag = broeser.find_element_by_id('loginname')
# input_tag.clear()
# input_tag.send_keys('15135544556')
# time.sleep(3)
# input_tag_pwd = broeser.find_element_by_xpath('//div[@class="info_list password"]/div[@class="input_wrap"]/input')
# input_tag_pwd.clear()
# input_tag_pwd.send_keys('123456789liu')
# time.sleep(3)
# button_tag = broeser.find_element_by_xpath('//div[@class="W_login_form"]/div[@class="info_list login_btn"]/a')
# button_tag.click()
```

## 5、图片懒加载

```python
'''
网址 http://sc.chinaz.com/tupian/   站长素材
图片懒加载
'''''

import requests
from lxml import etree
url = 'http://sc.chinaz.com/tupian/bingxueshijie.html'
headers = {
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36'
}
res = requests.get(url=url,headers=headers)
tree = etree.HTML(res.text)
#src2的位置，在必要的时候可以把值赋给src----懒加载核心
ret = tree.xpath('//div[@id="container"]/div/div/a/img/@src2')

for i in ret:
    comment = requests.get(url=i,headers=headers).content
    name = i.split('/')[-1]
    with open('./image/%s'% name,'wb') as f:
        f.write(comment)
```

# 八、Mongo数据库

## 1、mongo的安装

```python
(将安装目录下的bin目录添加到系统环境变量中)
# mongo的配置: 
1.创建数据库文件及日志文件目录, 并将目录写入配置文件 
2.在系统终端执行以下命令: mongod --bind_ip 127.0.0.1 --logpath "D:\Professional\MongoDB\log\mongodb.log" - -logappend --dbpath "D:\Professional\MongoDB\db" --port 27017 --serviceName "MongoDB" --serviceDisplayName "MongoDB" --install 3.将MongoDB服务设为开机自启
```

## 2、mongo数据库操作

```python
//常用操作: 
// 1.查看数据库 
show dbs 
// 2.创建并使用数据库 
use tst 
// 3.查看当前工作的数据库 
db
// 4.创建集合并插入一条数据 
db.goods.insert({"name":"辣条", "price":0.5}) 
// 5.查看所有表 
show tables 
// 6.查询表中所有数据 
db.goods.find() 
// 7.删除表操作
db.goods.drop() 
// 8.删除数据库 
db.dropDatabase()

```

```python
# 增加数据操作: 
db.tablename.insert({dict}) 
# 示例: 增加数据操作: 
db.goods.insert({"name":"辣条", "price":0.5}) 
db.goods.insert({"name":"辣条", "price":1}) 
db.goods.insert({"name":"干脆面", "price":0.5}) db.goods.insertOne({"name":"单身狗粮", "price":4.5}) db.goods.insertMany([{"name":"小洋人", "price":3.5}, {"name":"麦香鸡块", "price":5.5}])
```

```python
# 查询数据操作:
//1、查看数据库   show dbs只能查看有数据的数据库
show dbs

//2、创建并使用数据库
use a1903

//3、查看当前正在工作的数据库
db

//4、插入数据
db.student.insert({'name':'胡歌','age':30})

db.student.insertOne([{'name':'胡歌'}])
db.student.insertMany([{'name':'鹿晗','age':27},{'name':'关晓彤','age':23}])
db.student.insertMany([{'name':'刘国鑫','age':22},{'name':'庞明哲','age':20}])
//5、查询表中数据
db.student.find()
db.student.find().limit(3)


// 按条件查询
// 1.等值查询
db.student.find({'name':'晓彤'})

// 2.非等值查询：大于$gt(great than)，小于$lt(little than)，大于等于$gte(great than equal)，小于等于$lte(little than equal)，不等于$ne
db.student.find({age:{$gt:23}})
db.student.find({'age':{$lt:23}})
db.student.find({'age':{$ne:21}})

// 3. and 与 or 
db.student.find({'name':'晓彤','age':21})
db.student.find({$or[{'name':'晓彤'},{'age':27}]})
  
// and  a 与 b之间
db.student.find({'age':{$gt:20},'age':{$lte:27}})
db.student.find({'age':{$gt:20,$lte:27}})
 
db.student.find({$or:[{'age':{$lt:23}},{'age':{$gt:25}}]})

// 更新数据操作
db.student.update({'name':'晓彤'},{$set{'age':18}})

//6、查看表
show tables

//7、删除表
db.student.drop()

//8、删除库
db.dropDatabase()

//9、指定删除
db.student.remove({'name':''})
```

```python
# 更新数据操作: 
db.table.update({定位字典}, {指定修改的键值}) 
# 示例:更新数据操作: 
	db.goods.update({"price":0.5},{$set:{"price":5}}) 
		# 参数中的第一个字典用于定位要修改的数据 
		# 参数中的第二个字典是指定要更新已定位的数据 
		# 第二个参数中的字典是指定要将哪个字段的修改为什么
```

```python
# 删除数据操作: 
db.tablename.remove({定位字典})
# 示例:删除数据操作: 
db.goods.remove({"price":5})
```

## 3、Python与Mongo交互

```python
# 导入模块 如果没安装先安装  pip install pymongo
import pymongo 
# 连接MongoDB数据库 
conn = pymongo.MongoClient('localhost', 27017) 
# 创建库或连接数据库
db = conn.goods 
# 创建表或连接表
table = db.snacks 
# 数据操作: 插入数据 
table.insert(dict) 
table.insert_one(dict)  ******************在py文件中看具体清空使用
table.insert_many([dict1, dict2, dict3]) 
# 数据操作: 查询数据 
table.find_one({dict}) 
# 返回一个字典形式数据 
table.find() 
# 返回一个mongo对象, 需要使用for循环遍历取值 
table.find({dict}) # 同上
```

# 九、多线程爬虫

## 1、并发，并行的理解

```python
# 并发：同一时间段同时运行
# 并行：同一时刻同时运行
# 时间片轮转法：10个视屏不间断播放，是并发运行，但给人的错觉是并行
# 高IO密集(比如在一个刚运行的代码前就有设置的)  阻塞，cup算法密集
```

## 2、用代码实现多线程爬虫

```python
***用多线程爬虫，最重要的就是传参，获取数据，思路********
import threading,requests
# 导入多线程锁机制
from threading import Lock
# 导入线程队列
from queue import Queue
from lxml import etree
import pymongo
# 爬虫类,负责采集数据的
class CrawThread(threading.Thread):
    # 初始化init方法，接收参数
    def __init__(self,name,pageQueue,dataQueue):
        super().__init__()
        self.name = name
        self.pageQueue = pageQueue
        self.headers = {
            'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36'
        }
        self.dataQueue = dataQueue

    # 执行run方法，在def函数调用时执行
    def run(self):
        # 爬取的页数不确定，格式化输出
        base_url = 'https://www.xiaohua.com/duanzi?page=%s'
        while 1:
            try:
                print('开始url')
                # 传参时要抓取几页的数据已准备好直接get获取，block为False用try捕获异常抓取结束，就终止循环
                page = self.pageQueue.get(block=False)
                # 将页码拼接
                url = base_url % page
                res = requests.get(url=url,headers=self.headers)
                self.dataQueue.put(res.text)
                print('URL完成')
            except:
                break

# 数据解析类
class Parse(threading.Thread):
    def __init__(self,name,dataQueue,look):
        super().__init__()
        self.name = name
        self.dataQueue = dataQueue
        self.look = look

    def run(self):
        while 1:
            try:
                html = self.dataQueue.get(block=False)
                print('正在解析')
                # 把获取的HTML的文本放在下一个函数的里进行操作
                self.parserver(html)
                print('解析完毕')
            except:
                break
                
    def parserver(self,html):
        # 解析
        tree = etree.HTML(html)
        div_list = tree.xpath('//div[@class="one-cont"]')
        for div in div_list:
            item = {}
            author = div.xpath('./div/div/a/i/text()')
            item['author'] = author[0]
            # 上锁
            with self.look:
                self.save(item)

    def save(self,item):
        # 连接MongoDB数据库
        conn = pymongo.MongoClient('localhost',27017)
        db = conn.XIAOHUA
        table = db.xh
        table.insert_one(item)

def main():
    # 存放URl----实例化队列对象
    pageQueue = Queue()
    for j in range(1,11):
        # put将所需要的数据存入
        pageQueue.put(j)
    #存放脏数据
    dataQueue = Queue()
    crawlist = ['爬虫1号','爬虫2号','爬虫3号']
    # join等待进程以防有一个进程死掉
    for i in crawlist:
        c = CrawThread(i,pageQueue,dataQueue)
        c.start()
        c.join()
    # 实例化机制锁对象
    look = Lock()
    jiexi = ['解析1号','解析2号','解析3号',]
    for var2 in jiexi:
        cc = Parse(var2,dataQueue,look)
        cc.start()
        # 等待其他线程执行，以防提早结束其他线程死掉
        cc.join()


if __name__ == '__main__':
    main()
```

  # 十、scrapy框架

## 1、scrapy安装与环境依赖

```python
# 1.在安装scrapy前需要安装好相应的依赖库, 再安装scrapy, 具体安装步骤如下: 
(1).安装lxml库: pip install lxml 
(2).安装wheel: pip install wheel 
(3).安装twisted: pip install twisted文件路径 
    ***根据网址进入页面后，找到跟自己电脑相匹配的安装包下载，下载成功后，复制到一个文件夹在地址栏用cmd打开切换到python环境中执行命令  twisted: pip install twisted文件路径（T+tab键会自动生成）
	(twisted需下载后本地安装,下载地 址:http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted) 	  	  (版本选择如下图,版本后面有解释,请根据自己实际选择) 
(4).安装pywin32: pip install pywin32 
     (注意:以上安装步骤一定要确保每一步安装都成功,没有报错信息,如有报错自行百度解决) 
(5).安装scrapy: pip install scrapy 
     (注意:以上安装步骤一定要确保每一步安装都成功,没有报错信息,如有报错自行百度解决) 
(6).成功验证:在cmd命令行输入scrapy,显示Scrapy1.6.0-no active project,证明安装成功
```

## 2、创建项目等--命令介绍

```python
# 1.手动创建一个目录test 
# 2.在test文件夹下创建爬虫项目为spiderpro: *(命令)scrapy startproject spiderpro（项目名称） *
# 3.进入项目文件夹: cd spiderpro 
# 4.创建爬虫文件: scrapy genspider 爬虫名 域名（www.baidu.com---意思只能爬取在百度以内的东西）
# 5.启动scrapy的命令：scrapy crawl +'爬虫名'
# 6.解析方法 extract_first()--->目标数据，如果拼错了，不飘红也不报错，就是拿不到数据。
# 7.当在scrapy的框架中，获取列表数据的一条用---extract_first()
# 8.当在scrapy的框架中，获取列表的所有数据用---extract（）
```

## 3、项目目录介绍

```python
spiderpro 
	spiderpro # 项目目录 
	__init__ 
	spiders:爬虫文件目录 
		__init__ 
		tests.py:爬虫文件 
	items.py:定义爬取数据持久化的数据结构 
	middlewares.py:定义中间件 
	pipelines.py:管道,持久化存储相关 
	settings.py:配置文件 
venv:虚拟环境目录 
scrapy.cfg: scrapy 项目部署有关
    
#说明: 
    1).spiders:其内包含一个个Spider的实现, 每个Spider是一个单独的文件 
    2).items.py:它
    定义了Item数据结构, 爬取到的数据存储为哪些字段 
    3).pipelines.py:它定义Item Pipeline的实现
    4).settings.py:项目的全局配置 
    5).middlewares.py:定义中间件, 包括爬虫中间件和下载中间
    件 
    6).scrapy.cfg:它是scrapy项目的配置文件, 其内定义了项目的配置路径, 部署相关的信息等
```

## 4、框架scrapy介绍：五大核心组件与数据流向

```python
# 架构: 
1).Scrapy Engine: 这是引擎，负责Spiders、ItemPipeline、Downloader、Scheduler中间的通 讯，信号、数据传递等等! 
2).Scheduler(调度器): 它负责接受引擎发送过来的requests请求，并按照一定的方式进行整理排列， 入队、并等待Scrapy Engine(引擎)来请求时，交给引擎。 
3).Downloader（下载器)：负责下载Scrapy Engine(引擎)发送的所有Requests请求，并将其获取到 的Responses交还给Scrapy Engine(引擎)，由引擎交给Spiders来处理，
4).Spiders：它负责处理所有Responses,从中分析提取数据，获取Item字段需要的数据，并将需要跟进 的URL提交给引擎，再次进入Scheduler(调度器)， 
5).Item Pipeline：它负责处理Spiders中获取到的Item，并进行处理，比如去重，持久化存储（存数据 库，写入文件，总之就是保存数据用的） 
6).Downloader Middlewares(下载中间件)：你可以当作是一个可以自定义扩展下载功能的组件 
7).Spider Middlewares(Spider中间件)：你可以理解为是一个可以自定扩展和操作引擎和Spiders中 间‘通信‘的功能组件（比如进入Spiders的Responses;和从Spiders出去的Requests）


# 工作流: 
spider --> 引擎 --> 调度器 --> 引擎 --> 下载器 --> 引擎 --> spider --> 引擎 --> 管道 --> 数据库
    1).spider将请求发送给引擎, 引擎将request发送给调度器进行请求调度 
    2).调度器把接下来要请求的request发送给引擎, 引擎传递给下载器, 中间会途径下载中间件 
    3).下载携带request访问服务器, 并将爬取内容response返回给引擎, 引擎将response返回给 spider 
    4).spider将response传递给自己的parse进行数据解析处理及构建item一系列的工作, 最后将item 返回给引擎, 引擎传递个pipeline 	
    5).pipe获取到item后进行数据持久化 
    6).以上过程不断循环直至爬虫程序终止
    
    #__init__初始化方法   __new__()  构造方法 ：当spider接收到res响应后定义类，实例化对象存到属性中也就是存在内存上，下一步才准备存到数据库
```

## 5、scrapy--爬取科客网站

```python
# 1. itmes.py 里配置想要抓取的字段（想要抓取多少内容）

# -*- coding: utf-8 -*-

# Define here the models for your scraped items
#
# See documentation in:
# https://docs.scrapy.org/en/latest/topics/items.html

import scrapy
# 所写的字段
class ProItem(scrapy.Item):
    # define the fields for your item here like:
    img = scrapy.Field()    ******
    title = scrapy.Field()  ******
    image_url = scrapy.Field()   *****所需代码

    
# 2. 在自己定义的爬虫py文件中
# 定义了爬取数据的行为, 定义了数据解析的规则
import scrapy
#  将解析好的数据需要需要实例化存在一个属性中
from ..items import ProItem
class MyproSpider(scrapy.Spider):
    name = 'mypro'    # 爬虫名, 启动项目时用
    # allowed_domains = ['www.baidu.com']   # # 定义了爬取的范围 ，可注释掉，如果不注释则影响爬虫效果
    start_urls = ['http://www.keke289.com/']     ## 起始url, 项目启动时, 会自动向起始url发起请求
	#解析方法
    def parse(self, response):
        #用response响应进行数据解析--》xpath解析的并不是一个真实的列表
        div_list = response.xpath('//article[contains(@class,"article")]')
        for i in div_list:
            title = i.xpath('./div/h2/a/text()').extract_first()
            href = i.xpath('./div/h2/a/@href').extract_first()
            src = i.xpath('./div/a/img/@lazy_src').extract_first()
            #实例化对象
            item = ProItem()
            #将解析的数据找其相对应的字段进行赋值----存储到item属性中（字典）
            item['title'] = title
            item['image_url'] = href
            item['img'] = src
            # yield 将数据发送给管道Pipelines
            yield item
           
        
# 3.在pipelines.py 里进行MongoDB存储
# 导入pymongo模块
import pymongo
class ProPipeline(object):
    def process_item(self, item, spider):
        # python 与 MongoDB 数据库交互
        conn = pymongo.MongoClient('localhost',27017)
        # 创建或连接库
        db = conn.keke
        # 创建或连接表
        table = db.kuke
        table.insert_one(dict(item))
        return item
    
    
# 4. 在settinds.py 里修改所需要的配置
# 此示例中配置文件中的配置的项, 注意是不是全部的配置, 是针对该项目增加或修改的配置项 
# 忽略robots协议 -----》把True改成False(代表不遵守协议)
ROBOTSTXT_OBEY =False 
# UA伪装 ----》换成自己的 UA
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.87 Safari/537.36' 
# 管道类打开以防数据发送不过来 --->把注释解掉即可
ITEM_PIPELINES ={ 'qsbk.pipelines.QsbkPipeline':300, }

```

## 6、scrapy实现多页爬取

```python
1). #在起始url中，输入有顺序的每页地址，然后格式化输出。
例：
start_urls = ['http://www.009renti.com/evarenti/RenTiCaiHui/14_%s.html' % for i in range(1,3)]


2). # spider编码在原基础之上, 构建其他页面的url地址, 并利用scrapy.Request发起新的请求, 请求的 回调函数依然是parse: 
#声明page页码数为第一页
page = 1 
base_url = 'http://www.xiaohuar.com/list-1-%s.html' 
# 爬取一共四页的数据所以小于4，因为每次self.page+1,当到第三页时依次加一所以就四页了，要几页写几页
if self.page < 4: 
    #格式化拼接页码数
    page_url = base_url%self.page 
    #依次加一
    self.page += 1 
    #爬取多页收据，将yield函数里  用scrapy.Request再次请求，callback回调其自身函数就OK
    yield scrapy.Request(url=page_url, callback=self.parse) 
    # (其他文件不用改动)
   
####  准确使用
import scrapy
from ..items import BizhiItem
class MybizhiSpider(scrapy.Spider):
    name = 'mybizhi'
    # allowed_domains = ['www.baidu.com']
    1).# start_urls = ['http://sj.zol.com.cn/bizhi/mingxing/%s.html'% i for i in range(1,5)]
    2). #定义起始url为第一页数据
    start_urls = ['http://sj.zol.com.cn/bizhi/mingxing/1.html']
    #定义page=1从第一页开始
    page = 1
    def parse(self, response):
        div_list = response.xpath('//li[@class="photo-list-padding"]')
        for div in div_list:
            title = div.xpath('./a/span/em/text()').extract_first()
            image_url = div.xpath('./a/img/@src').extract_first()
            detail_url = div.xpath('./a/@href').extract_first()
            item = BizhiItem()
            item['title'] = title
            item['image_url'] = image_url
            item['detail_url'] = detail_url
            # 输入爬取多少页的数据
            if self.page < 4:
                #每次加一页，直到全部爬取结束
                self.page += 1
                #将每页拼接到页码上
                url = 'http://sj.zol.com.cn/bizhi/mingxing/%s.html'%self.page
                #再次请求，调用自身函数
                yield scrapy.Request(url=url,callback=self.parse)
            #最后将数据发送到管道，存入MongoDB
            yield item
```

## 7、scrapy解析笑话网站例

```python
import scrapy
from ..items import SkillItem

class MyskillSpider(scrapy.Spider):
    name = 'myskill'
    # allowed_domains = ['www.baidu.com']
    start_urls = ['http://www.jokeji.cn/?bmjmxa=ziqzh']
	#自己定义的函数
    def detail_parse(self, response):
        # 将回调传来的值，取出
        item = response.meta['item']
        *****
        当在scrapy的框架中，获取列表数据的一条用---extract_first()
        当在scrapy的框架中，获取列表的所有数据用---extract（）
        *****
        detail_url = response.xpath('//span[@id="text110"]/p/text()').extract()
        # 解析的目标数据是一个大字典用  '' .join()拼接使其变成字符串
        item['detail_url'] = ''.join(detail_url)
        # 最后在发送到管道
        yield item

    def parse(self, response):
        div_list = response.xpath('//div[@class="newcontent l_left"]/ul/li')
        for div in div_list:
            title = div.xpath('./a/text()').extract_first()
            link = div.xpath('./a/@href').extract_first()
            item = SkillItem()
            item['title'] = title
            item['link'] = 'http://www.jokeji.cn'+link
            # 爬取网页xpath解析，实例化对象将其相对应的字段进行赋值，获取详情页连接再次发起去请求，
            # 用yield的SCRAPY.Request的内置参数callback回调一个函数，meta将值传送到回调函数
            yield scrapy.Request(url='http://www.jokeji.cn'+link,callback=self.detail_parse,meta={'item':item})
```

## 8、scrapy框架下载图片代码

```python
1).item.py定义字段赋值

import scrapy
class BizhiItem(scrapy.Item):
    # 定义需要的字段与爬虫文件相关联
    title = scrapy.Field()
    image_url = scrapy.Field()
    detail_url = scrapy.Field()
    
2). 爬虫py文件

import scrapy
# 导入item的类名内容
from ..items import BizhiItem
class MybizhiSpider(scrapy.Spider):
    name = 'mybizhi'
    # allowed_domains = ['www.baidu.com']
    # start_urls = ['http://sj.zol.com.cn/bizhi/mingxing/%s.html'% i for i in range(1,5)]
    start_urls = ['http://sj.zol.com.cn/bizhi/mingxing/1.html']
    def pic_parse(self, response):
        #接收传送的参数取出来
        item = response.meta['item']
        #图片的名字按照 / 切割
        name = item['image_url'].split('/')[-1]
        # 响应内容，获取图片的二进制流
        content = response.body
        # open 打开的文件，***************imgs文件（图片存储的文件）一定要跟scrapy.cfg平级，不然拿不到***********
        with open('./imgs/%s'%name,'wb') as f:
            #把二进制流写入
            f.write(content)
            # 逻辑工作结束后，最后将item发送至管道
            yield item

    def parse(self, response):
        div_list = response.xpath('//li[@class="photo-list-padding"]')
        for div in div_list:
            title = div.xpath('./a/span/em/text()').extract_first()
            image_url = div.xpath('./a/img/@src').extract_first()
            detail_url = div.xpath('./a/@href').extract_first()
            item = BizhiItem()
            item['title'] = title
            item['image_url'] = image_url
            item['detail_url'] = detail_url
            # 再次请求一个图片地址链接，把赋值好的属性传送过去。
            yield scrapy.Request(url=image_url,callback=self.pic_parse,meta={'item':item}) 
            
3).piplines.py 管道py存数据
import pymongo

class BizhiPipeline(object):
    def process_item(self, item, spider):
        # 与MongoDB数据库交互   域名加端口
        conn = pymongo.MongoClient('localhost',27017)
        # 创建数据库或者连接数据库
        db = conn.xxxxx
        # 创建表或者连接表
        table = db.yyyyy
        # 插入数据
        table.insert_one(dict(item))
        return item

     
4).settings.py配置内容
# 此示例中配置文件中的配置的项, 注意是不是全部的配置, 是针对该项目增加或修改的配置项 
# 忽略robots协议 -----》把True改成False(代表不遵守协议)
ROBOTSTXT_OBEY =False 
# UA伪装 ----》换成自己的 UA
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.87 Safari/537.36' 
# 管道类打开以防数据发送不过来 --->把注释解掉即可
ITEM_PIPELINES ={ 'qsbk.pipelines.QsbkPipeline':300, }
```

## 9、scrapy 篡改请求与响应，item丢弃

```python
1).-- UA池--->大量UA----->拦截请求  ---->换UA 
2).-- IP代理池---->请求---->换IP
3).-- cookie池---->换cookie
4).-- 拦截响应（动态加载）--selenium抓取（res.scrapy<---->res.selenium）--给引擎--->spider
```

# 10、scrapy中间件--分类，作用

```python
# 中间件分类
	- 下载中间键：DownloadMiddleware
    - 爬虫中间件：SpiderMiddleware

# 中间件的作用
	- 下载中间件: 拦截请求与响应, 篡改请求与响应 
    - 爬虫中间件: 拦截请求与响应, 拦截管道item, 篡改请求与响应, 处理item

# 下载中间件的主要方法: 
process_request     #获取拦截非异常请求
process_response    #获取拦截所有响应
process_exception   #获取拦截异常请求
```

## 11、下载中间件拦截请求, 使用代理ip案例 

```python
1).# spider编码: 
import scrapy 
class DlproxySpider(scrapy.Spider): 
	name = 'dlproxy' 
	# allowed_domains = ['www.baidu.com'] 
	start_urls = ['https://www.baidu.com/s?wd=ip']
def parse(self, response): 
	with open('baiduproxy.html', 'w', encoding='utf-8') as f: 
		f.write(response.text)
        
2).# Downloadermiddleware编码: 
def process_request(self, request, spider): 
    # http://www.goubanjia.com
    request.meta['proxy'] = 'http://111.231.90.122:8888' 
    return None

3).# settings编码
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36'
ROBOTSTXT_OBEY = False
# 把下载中间件注释打开
Downloader_MIDDLEWARES = {
   'proxy.middlewares.ProxySpiderMiddleware': 543,
}
```

## 12、下载中间件实现UA池

```python
1). 在middlewares.py
from scrapy import signals
# ua导包需下载 pip install fake-useragent

from fake_useragent import UserAgent
#导入随机
import random
#实例化
ua_chrome = UserAgent()
#定义ua池
ua_pool = []
for i in range(10):
    ua = ua_chrome.Chrome
    ua_pool.append(ua)
    
    # 拦截请求：拦截非异常的请求
    def process_request(self, request, spider):
        # request.meta['proxy'] = 'http://60.217.64.237:38829'
        request.headers['User-Agent'] = random.choice(ua_pool)
        return None
    # 拦截响应：拦截的是所有响应
    def process_response(self, request, response, spider):
        print('*'*50)
        ***** request.headers['User-Agent'] *****取ua
        print(request.headers['User-Agent'])
        print('*'*50)
        return response
   
2).  需要注释的地方，跟修改的地方  在settings.py 里设置
1.USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36'
2.ROBOTSTXT_OBEY = False
#把下载中间件注释开
3.DOWNLOADER_MIDDLEWARES = {
   'proxy.middlewares.ProxyDownloaderMiddleware': 543,
}

3). #在起始url里，用列表推导式里面加入for循环，依次循环就可以在ua池里任意获取
start_urls = ['https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&rsv_idx=1&tn=baidu&wd=ip&rsv_pq=cf95e45f000b8d2b&rsv_t=74b1V5e7UWXPDK6YWqzjFSXv%2B9wpMSDHZrF4HMP0TnouyBZ4o6hj%2FuiRWgI&rqlang=cn&rsv_enter=1&rsv_dl=tb&rsv_sug3=2&rsv_sug1=1&rsv_sug7=100&rsv_sug2=0&inputT=1452&rsv_sug4=1453' for i in range(3)]

*************************************************************
简单的UA池
from fake_useragent import UserAgent
for i in range(10):
    USER_AGENT = UserAgent().random
    print(USER_AGENT)
*************************************************************
```

## 13、selenium与scrapy框架对接

```python
1).item.py里
# 定义爬取的字段
import scrapy
class NewsItem(scrapy.Item):
    title = scrapy.Field()
    image_url = scrapy.Field()
    
*****************************************************************************************
2).在爬虫的py里
import scrapy
from ..items import NewsItem
from selenium import webdriver
class MynewsSpider(scrapy.Spider):
    name = 'mynews'
    # allowed_domains = ['www.baidu.com']
    # 在起始的url里定义网址爬取的范围
    start_urls = ['https://news.163.com/domestic/']
    # 实例化selenium对象，executable_path=‘驱动程序的工具路径’
    browser = webdriver.Chrome(executable_path=r'D:\爬虫段位\day13\news\chromedriver.exe')

    def image_parse(self, response):
        item = response.meta['item']
        content = response.body
        name = item['image_url'].split('/')[-1].split('?')[0]
        with open('./imgs/%s'% name,'wb') as f:
            f.write(content)
            yield item

    def parse(self, response):
        # 正常的xpath解析
        div_list = response.xpath('//div[contains(@class,"news_article")]')
        for div in div_list:
            title = div.xpath('./div/div/h3/a/text()').extract_first()
            image_url = div.xpath('./a/img/@src').extract_first()
            item = NewsItem()
            item['title'] = title
            item['image_url'] = image_url
            yield scrapy.Request(url=image_url,callback=self.image_parse,meta={'item':item})

*****************************************************************************************            
3).pipelines.py里 存数据
import pymongo

class NewsPipeline(object):
    def process_item(self, item, spider):
        conn = pymongo.MongoClient('localhost',27017)
        db = conn.news
        table = db.wynews
        table.insert_one(dict(item))
        return item
    
***************************************************************************************** 
4).middlewares.py里
from scrapy import signals
from scrapy.http import HtmlResponse
# 因为获取js的动态数据所以属于响应拦截
    def process_response(self, request, response, spider):
        # 在爬虫py里已导入自动化工具，用spider导进来运用
        browser = spider.browser
        # 在第一次请求的js动态数据，判断拦截后的响应的url在不在起始url里
        #   注意爬取的连接， 必要时判断
        if response.url in spider.start_urls:
            # 用自动化工具进行请求页面
            browser.get(request.url)
            # js下拉框 下拉一次
            js = 'window.scrollTo(0,document.body.scrollHeight)'
            # 把js 代码放入 browser.execute_script(js)
            browser.execute_script(js)
            # 获取响应后的页面赋给变量
            html = browser.page_source
            # 将拦截后获取的数据在发送给爬虫文件解析----
            # 固定参数 url=browser.current_url（currnet_url代表当前请求的url）,body=html（body请求体）,
            # encoding='utf-8'（文本编码）,request=request（伪装成请求头，返回爬虫网页））

            return HtmlResponse(url=browser.current_url,body=html,encoding='utf-8',request=request)
        return response
    
*****************************************************************************************
5).settings.py设置
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36'
ROBOTSTXT_OBEY = False
DOWNLOADER_MIDDLEWARES = {
   'news.middlewares.NewsDownloaderMiddleware': 543,
}
ITEM_PIPELINES = {
   'news.pipelines.NewsPipeline': 300,
}
```

## 14、scrapy 持久化与MongoDB交互

```python
# 核心方法讲解: 
open_spider(self, spider): spider开启是被调用 close_spider(self, spider): spider关闭是被调用 from_crawler(cls, crawler): 类方法, 用@classmethod标识, 可以获取配置信息 
Process_item(self, item, spider): 与数据库交互存储数据, 该方法必须实现 
***** # 重点: 所有的方法名都必须一致

1). #在管道pipelines.py
import pymongo
class XiaoxiaoPipeline(object):
    # 初始化方法, __new__: 构造方法, 在内存中开辟一块空间
    def __init__(self,mongo_uri,mongo_db):
        self.mongo_uri = mongo_uri
        self.mongo_db = mongo_db

    def open_spider(self,spider):
        self.client = pymongo.MongoClient(self.mongo_uri)
        self.db = self.client[self.mongo_db]

    @classmethod
    # 调用配置中定义的方法
    def from_crawler(cls,crawler):
        return cls(
            mongo_uri = crawler.settings.get('MONGO_URI'),
            mongo_db = crawler.settings.get('MONGO_DB')
        )


    def process_item(self, item, spider):
        self.db['myxiao'].insert(dict(item))
        return item

    def close_spider(self,spider):
        self.client.close()
        
2). # 爬虫文件正常爬取思路
import scrapy
from ..items import XiaoxiaoItem
class MyxiaoSpider(scrapy.Spider):
    name = 'myxiao'
    # allowed_domains = ['www.baidu.com']
    start_urls = ['http://duanziwang.com/']

    def parse(self, response):
        div_list = response.xpath('//article[@class="post"]')
        for div in div_list:
            title = div.xpath('./div/h1/a/text()').extract_first()
            cont = div.xpath('./div[2]/p/text()').extract()
            content = ''.join(cont)
            item = XiaoxiaoItem()
            item['title'] = title
            item['content'] = content
            yield item
            
3). # settings.py 配置

USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36'

ROBOTSTXT_OBEY = False

ITEM_PIPELINES = {
    # 跟中间件的类相对应，后面的值，谁小谁先运行
   'xiaoxiao.pipelines.XiaoxiaoPipeline': 300,
}

MONGO_URI = 'localhost'
MONGO_DB = 'xiaoxiao'

```

## 15、scrapy 持久化与Mysql交互

```python
1). # 在中间管道.py里
import pymysql
class MyXiaoxiaoPipeline(object):
    def __init__(self,host,database,user,password,port):
        self.host = host
        self.database = database
        self.user = user
        self.password = password
        self.port = port

    def open_spider(self,spider):
        self.client = pymysql.connect(self.host,self.user,self.password,self.database,charset='utf8',port=self.port)
        self.corsor = self.client.cursor()
	

    @classmethod
    def from_crawler(cls,crawler):
        return cls(
            host=crawler.settings.get('MYSQL_HOST'),
            database = crawler.settings.get('MYSQL_DATABASE'),
            user = crawler.settings.get('MYSQL_USER'),
            password = crawler.settings.get('MYSQL_PASSWORD'),
            port = crawler.settings.get('MYSQL_PORT')
        )


    def process_item(self, item, spider):
        
        
        data = dict(item)
        # data.keys()---->获取所有的键，字段----（title，content）
        keys = ','.join(data.keys())
        # 获取所有的值
        values = ','.join(['%s']*len(data))
        sql = 'insert into %s (%s) values (%s)'% ('myxiao',keys,values)
        self.corsor.execute(sql,tuple(data.values()))
        # 提交
        self.client.commit()
        return item
    
2).在settings.py 里
ITEM_PIPELINES = {
   'xiaoxiao.pipelines.MyXiaoxiaoPipeline': 295,
}

MYSQL_HOST = 'localhost'
MYSQL_DATABASE = 'xiaoxiao'
MYSQL_USER = 'root'
MYSQL_PASSWORD = ''
MYSQL_PORT = 3306

ROBOTSTXT_OBEY = False

USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36'

```

## 16、基于crawlSpider的全站数据爬取

```python
1).# 项目的创建 
scrapy startproject projectname 
scrapy genspider -t crawl spidername www.baidu.com

2).# crawlspider全站数据爬取: 
- CrawlSpider是一个爬虫类, 是scrapy.spider的子类, 功能比spider更强大. 
- CrawlSpider的机制: 
    - 连接提取器: 可以根据指定的规则进行连接的提取 
    - 规则解析器: 根据指定的规则对响应数据进行解析
        
3))# 案例: 基于CrawlSpider对笑话网进行全站深度数据爬取, 抓取笑话标题与内容, 并存储于MongoDB中
# item编码: 
import scrapy 
class JokeItem(scrapy.Item): 
    title = scrapy.Field() 
    content = scrapy.Field()
    
# spider编码: 
import scrapy 
from scrapy.linkextractors import LinkExtractor 
from scrapy.spiders import CrawlSpider, Rule 
from..items import JokeItem 
class ZSpider(CrawlSpider): 
    name = 'z' 
    # allowed_domains = ['www.baidu.com'] 
    start_urls = ['http://xiaohua.zol.com.cn/lengxiaohua/'] 
    link = LinkExtractor(allow=r'/lengxiaohua/\d+.html') 
    link_detail = LinkExtractor(allow=r'.*?\d+\.html') 
    rules = ( Rule(link, callback='parse_item', follow=True), 		                   Rule(link_detail, callback='parse_detail'), )
    def parse_item(self, response): 
        pass 
    def parse_detail(self, response): 
        title = response.xpath('//h1[@class="article- title"]/text()').extract_first() 
        content = response.xpath('//div[@class="article- text"]//text()').extract() 
        content = ''.join(content) 
        if title and content: 
            item = JokeItem() 
            item["title"] = title 
            item["content"] = content 
            print(dict(item)) 
            yield item
            
# pipeline编码: 
class JokePipeline(object): 
    def __init__(self, mongo_uri, mongo_db): 
        self.mongo_uri = mongo_uri 
        self.mongo_db = mongo_db 
    @classmethod 
    def from_crawler(cls, crawler): 
        return cls( 
            mongo_uri=crawler.settings.get('MONGO_URI'), 			                 				mongo_db=crawler.settings.get('MONGO_DB') ) 
    def open_spider(self, spider):
        self.client = pymongo.MongoClient(self.mongo_uri) 
        self.db = self.client[self.mongo_db] 
    def process_item(self, item, spider):                          				 	             self.db["joke"].insert(dict(item)) 
        return item 
    def close(self, spider): 
        self.client.close()
```

## 17、增量式爬虫

```python
# 概念: 
- 检测网站数据更新, 只爬取更新的内容 
- 核心: 
    去重 
    - url 
    - 数据指纹
# 增量式爬虫: 电影名称与电影类型的爬取 # url: https://www.4567tv.co/list/index1.html

#item.py
import scrapy
class MoveItem(scrapy.Item):
    title = scrapy.Field()
    lab = scrapy.Field()
#爬虫py文件
import scrapy
from ..items import MoveItem

from redis import Redis
class MymoveSpider(scrapy.Spider):
    name = 'mymove'
    # allowed_domains = ['www.baidu.com']
    start_urls = ['https://www.4567tv.co/list/index1.html']
    # 连接redis数据库
    conn = Redis('localhost',6379)
    def detail_parse(self, response):
        title = response.xpath('//div[@class="ct-c"]/dl/dt/text()').extract_first()
        lab = response.xpath('//div[@class="ee"]/text()').extract_first()
        item = MoveItem()
        item['title'] = title
        item['lab'] = lab
        yield item
    def parse(self, response):
        link = response.xpath('//div[contains(@class,"index-area")]/ul/li/a/@href').extract()
        for i in link:
            # 如果link已经存在, 则ret为0, 说明该数据爬过来
            # 如果link不存在, 则ret为1, 说明没爬虫
            ret = self.conn.sadd('link',i)   # 把需要的数据存入redis库
            #  反向思绪
            if ret:
                print('有新数据, 可以爬取---------------------------------')
                yield scrapy.Request(url='https://www.4567tv.co'+i,callback=self.detail_parse)
            else:
                print('没有数据更新, 不需要爬取###############################')
                
# pipelines.py
import pymongo

class MovePipeline(object):
    def process_item(self, item, spider):

        conn = pymongo.MongoClient('localhost',27017)
        db = conn.move
        table = db.mv
        table.insert_one(dict(item))
        return item
    
# settings.py
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36'
ROBOTSTXT_OBEY = False
ITEM_PIPELINES = {
   'move.pipelines.MovePipeline': 300,
}
```

## 18、redis安装

```python
1.将安装包解压到一个文件夹下: 如 D:\redis, 会在该文件夹下看到所有redis的文件 
2.将该文件夹添加至系统环境变量中 
3.在解压的文件目录的地址栏上输入cmd, 在cmd窗口中输入redis-server ./redis.windows.conf , 然后回车, 如果出现下面图片的样子说明redis安装成功了

*****************************************************************************************
启动redis服务端
	cmd:redis-server ./redis.windows.conf
启动redis客户端
	cmd:redis-cli
    # 查询所有键   
    keys *
    #  删除键
    del 键名
    # 添加数据
    sadd name ()
    # 查询所有数据
    lrange xw:item 0 -1
    # 存储
    set name laowang
OK	# 取出来
127.0.0.1:6379> get name
"laowang"
127.0.0.1:6379>
```

# 19、mongo的分组聚合统计用django显示

```python
# 连接Mongo 数据库
import pymongo
conn = pymongo.MongoClient('localhost',27017)
db = conn.fqxh
table = db.xh

def login(request):
    # find（）查询所有的数据
    res = table.find()
    return render(request,'aaa.html',locals())

def index(request):
    # 对都需要的值进行排序ASC是正序，DESC倒叙
    res = table.find().sort([('times_date',pymongo.ASCENDING)])
    return render(request,'index.html',locals())


def indexs(request):
    ret = table.aggregate([{'$group':{'_id':'$times_date','cc':{'$sum':'$count'}}}])
    li  = []
    for i in ret:
        i['date'] = i['_id']
        li.append(i)
    return render(request,'indexs.html',locals())

def total(request):
    # Mongo的分组聚合统计，按照日期分
    res = table.aggregate([{'$group':{'_id':'$times_date','cc':{'$sum':1}}}])
    li = []
    for i in res:
        i['date'] = i['_id']
        li.append(i)
    return render(request,'ccc.html',locals())
```

# 十一、你了解scrapy的中间件：（广度，深度）



```python
答：（1）：是什么
    （2）：做什么：ua伪装，IP篡改，模拟登陆，与selenium框架对接爬取动态数据
    （3）：业务场景：
   

# 中间件
1).ua篡改    
		下载中间件   
2).IP篡改
3).selenium抓取数据


https://www.jianshu.com/p/32ac6def6a6b
并发：同一时间段同时运行

并行：同一时刻同时运行

时间片轮转法：10个视屏不间断播放，是并发运行，但给人的错觉是并行

高IO密集  阻塞，cup算法密集

多线程导致数据不安全



block：标定队列在空或满时，进入下一个操作状态是否进入阻塞状态

```
